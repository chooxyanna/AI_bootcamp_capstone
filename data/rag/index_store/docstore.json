{"docstore/ref_doc_info": {"455ecf91-8725-44da-8d55-7df62fe53157": {"node_ids": ["898c6214-2aed-4be5-bb9c-5c1198fb991c"], "metadata": {"file_name": "__init__.py"}}, "570c4da5-2013-42e2-bea3-e6c14ed46b5c": {"node_ids": ["c81b9ddb-d3ab-4f04-9e98-ee4a6977d789"], "metadata": {"file_name": "indexer.py"}}}, "docstore/data": {"898c6214-2aed-4be5-bb9c-5c1198fb991c": {"__data__": {"id_": "898c6214-2aed-4be5-bb9c-5c1198fb991c", "embedding": null, "metadata": {"file_name": "__init__.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455ecf91-8725-44da-8d55-7df62fe53157", "node_type": "4", "metadata": {"file_name": "__init__.py"}, "hash": "e84e3d1dcf587de001e7a4eb0fa821cc21c4317ce5934edbeb330cbdec1000e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# app/rag/__init__.py\r\nfrom .indexer import QUERY_ENGINE, format_sources, build_or_load_index\r\n\r\n__all__ = [\"QUERY_ENGINE\", \"format_sources\", \"build_or_load_index\"]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c81b9ddb-d3ab-4f04-9e98-ee4a6977d789": {"__data__": {"id_": "c81b9ddb-d3ab-4f04-9e98-ee4a6977d789", "embedding": null, "metadata": {"file_name": "indexer.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "570c4da5-2013-42e2-bea3-e6c14ed46b5c", "node_type": "4", "metadata": {"file_name": "indexer.py"}, "hash": "c070ad4f8eed2636afbd4606e211be9d96ebbd207a3afc03fa396a937c27773a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# app/rag/indexer.py\r\nimport os, shutil, json, hashlib, time\r\nfrom collections import defaultdict\r\nfrom typing import Dict, Any, List\r\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\r\nfrom llama_index.core import StorageContext, load_index_from_storage\r\nfrom llama_index.core.node_parser import SentenceSplitter\r\nfrom app.config import RAG_DIR, INDEX_DIR\r\n\r\nMANIFEST_NAME = \"manifest.json\"\r\n\r\ndef _walk_snapshot(doc_dir: str) -> Dict[str, Dict[str, Any]]:\r\n    \"\"\"\r\n    Walk doc_dir and return {rel_path: {size, mtime}}.\r\n    Only files (skip directories). Works for PDFs, txt, etc.\r\n    \"\"\"\r\n    snap = {}\r\n    for root, _, files in os.walk(doc_dir):\r\n        for f in files:\r\n            p = os.path.join(root, f)\r\n            try:\r\n                st = os.stat(p)\r\n            except FileNotFoundError:\r\n                continue\r\n            rel = os.path.relpath(p, doc_dir)\r\n            snap[rel] = {\"size\": st.st_size, \"mtime\": int(st.st_mtime)}\r\n    return snap\r\n\r\ndef _load_manifest(index_dir: str) -> Dict[str, Any]:\r\n    path = os.path.join(index_dir, MANIFEST_NAME)\r\n    if os.path.exists(path):\r\n        with open(path, \"r\", encoding=\"utf-8\") as f:\r\n            return json.load(f)\r\n    return {}\r\n\r\ndef _save_manifest(index_dir: str, snapshot: Dict[str, Any]) -> None:\r\n    os.makedirs(index_dir, exist_ok=True)\r\n    path = os.path.join(index_dir, MANIFEST_NAME)\r\n    with open(path, \"w\", encoding=\"utf-8\") as f:\r\n        json.dump({\"snapshot\": snapshot, \"saved_at\": int(time.time())}, f, ensure_ascii=False, indent=2)\r\n\r\ndef _has_changed(doc_dir: str, index_dir: str) -> bool:\r\n    cur = _walk_snapshot(doc_dir)\r\n    man = _load_manifest(index_dir)\r\n    prev = man.get(\"snapshot\", {})\r\n    return cur != prev\r\n\r\ndef _build_index(doc_dir: str, index_dir: str) -> VectorStoreIndex:\r\n    # Read documents (PDFs, txt, md, etc. supported by SimpleDirectoryReader)\r\n    documents = SimpleDirectoryReader(doc_dir).load_data()\r\n\r\n    # Merge pages per file\r\n    file_to_pages = defaultdict(list)\r\n    for d in documents:\r\n        fname = d.metadata.get(\"file_name\") or d.metadata.get(\"filename\") or \"unknown_source\"\r\n        file_to_pages[fname].append(d.text)\r\n\r\n    merged_docs: List[Document] = [\r\n        Document(text=\"\\n\".join(pages), metadata={\"file_name\": fn})\r\n        for fn, pages in file_to_pages.items()\r\n    ]\r\n\r\n    # Chunk\r\n    splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=32)\r\n    nodes = splitter.get_nodes_from_documents(merged_docs) if merged_docs else []\r\n\r\n    # Build + persist\r\n    index = VectorStoreIndex(nodes)\r\n    os.makedirs(index_dir, exist_ok=True)\r\n    index.storage_context.persist(persist_dir=index_dir)\r\n\r\n    # Save manifest snapshot\r\n    _save_manifest(index_dir, _walk_snapshot(doc_dir))\r\n    return index\r\n\r\ndef build_or_load_index(doc_dir: str = str(RAG_DIR),\r\n                        index_dir: str = str(INDEX_DIR)) -> VectorStoreIndex:\r\n    \"\"\"\r\n    Load an existing index if present and up-to-date; otherwise rebuild.\r\n    If loading fails, nuke the index dir and rebuild cleanly.\r\n    \"\"\"\r\n    os.makedirs(doc_dir, exist_ok=True)\r\n\r\n    if os.path.exists(index_dir):\r\n        # If docs changed since last build, rebuild.\r\n        if _has_changed(doc_dir, index_dir):\r\n            shutil.rmtree(index_dir, ignore_errors=True)\r\n            return _build_index(doc_dir, index_dir)\r\n        # Try to load existing\r\n        try:\r\n            storage = StorageContext.from_defaults(persist_dir=index_dir)\r\n            return load_index_from_storage(storage)\r\n        except Exception:\r\n            shutil.rmtree(index_dir, ignore_errors=True)\r\n            return _build_index(doc_dir, index_dir)\r\n    else:\r\n        # No index yet -> build\r\n        return _build_index(doc_dir, index_dir)\r\n\r\n# --- singletons ---\r\n_INDEX = build_or_load_index()\r\nQUERY_ENGINE = _INDEX.as_query_engine(similarity_top_k=5)\r\n\r\ndef format_sources(resp):\r\n    out = []\r\n    for n in getattr(resp, \"source_nodes\", []) or []:\r\n        out.append({\r\n            \"file\": n.metadata.get(\"file_name\", \"unknown\"),\r\n            \"score\": getattr(n, \"score\", None),\r\n            \"preview\": (n.text[:240] + \"\u2026\") if n.text else \"\"\r\n        })\r\n    return out", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"898c6214-2aed-4be5-bb9c-5c1198fb991c": {"doc_hash": "249762d7488d34a1f8f7ce5701841bd00307a338a9d4cbc4c25cec5c0fa34b66", "ref_doc_id": "455ecf91-8725-44da-8d55-7df62fe53157"}, "c81b9ddb-d3ab-4f04-9e98-ee4a6977d789": {"doc_hash": "fc2c75a7ba3fb6a3cc1e368b4c87b0a4119de9252798ef315c521a8e9f3e9518", "ref_doc_id": "570c4da5-2013-42e2-bea3-e6c14ed46b5c"}}}